{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Extending Logistic Regression\n",
    "\n",
    "### Authors: *Will Lahners, Edward Powers, and Nino Castellano*\n",
    "### 1) Preperation and Overview\n",
    "#### 1.1) Business Use Case\n",
    "\n",
    "Prediction Task: \n",
    "\n",
    "Buisness Case:\n",
    "\n",
    "Model Deployment:\n",
    "\n",
    "Model Accuracy:\n",
    "\n",
    "#### 1.2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2.970060</td>\n",
       "      <td>2.997060</td>\n",
       "      <td>2.902558</td>\n",
       "      <td>2.902558</td>\n",
       "      <td>2.349932</td>\n",
       "      <td>55057948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>2.920559</td>\n",
       "      <td>2.979060</td>\n",
       "      <td>2.916059</td>\n",
       "      <td>2.956560</td>\n",
       "      <td>2.393652</td>\n",
       "      <td>31801480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>3.006061</td>\n",
       "      <td>3.006061</td>\n",
       "      <td>2.965560</td>\n",
       "      <td>2.992560</td>\n",
       "      <td>2.422798</td>\n",
       "      <td>37002206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>2.988060</td>\n",
       "      <td>2.988060</td>\n",
       "      <td>2.929559</td>\n",
       "      <td>2.938559</td>\n",
       "      <td>2.379078</td>\n",
       "      <td>30158007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>2.866558</td>\n",
       "      <td>2.889058</td>\n",
       "      <td>2.857558</td>\n",
       "      <td>2.875558</td>\n",
       "      <td>2.328072</td>\n",
       "      <td>83954148.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close      Volume\n",
       "0  2015-02-17  2.970060  2.997060  2.902558  2.902558   2.349932  55057948.0\n",
       "1  2015-02-18  2.920559  2.979060  2.916059  2.956560   2.393652  31801480.0\n",
       "2  2015-02-19  3.006061  3.006061  2.965560  2.992560   2.422798  37002206.0\n",
       "3  2015-02-20  2.988060  2.988060  2.929559  2.938559   2.379078  30158007.0\n",
       "4  2015-02-23  2.866558  2.889058  2.857558  2.875558   2.328072  83954148.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# read the CSV file\n",
    "data = pd.read_csv('stockpriceforecast/data/prices.csv')\n",
    "\n",
    "# display the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the dataset primarily consists of stock prices and volume, scaling may not be strictly necessary, especially if interpretability is a concern. However, scaling could still be beneficial for ensuring that all features have similar ranges and for potentially improving convergence speed.\n",
    "\n",
    "Therefore, my recommendation would be to scale the data as a precautionary measure to ensure consistent behavior across different datasets and to potentially improve the model's performance. However, if interpretability is a top priority and you don't observe convergence issues during model training, you could choose to not scale the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types: Date          object\n",
      "Open         float64\n",
      "High         float64\n",
      "Low          float64\n",
      "Close        float64\n",
      "Adj Close    float64\n",
      "Volume       float64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      " Date         0\n",
      "Open         1\n",
      "High         1\n",
      "Low          1\n",
      "Close        1\n",
      "Adj Close    1\n",
      "Volume       1\n",
      "dtype: int64\n",
      "\n",
      "Final dataset overview:\n",
      "               Open          High           Low         Close     Adj Close  \\\n",
      "count  2.056000e+03  2.056000e+03  2.056000e+03  2.056000e+03  2.056000e+03   \n",
      "mean   1.105903e-16 -1.658855e-16  1.105903e-16  2.211806e-16 -1.658855e-16   \n",
      "std    1.000243e+00  1.000243e+00  1.000243e+00  1.000243e+00  1.000243e+00   \n",
      "min   -7.369443e-01 -7.338389e-01 -7.615335e-01 -7.356884e-01 -6.718673e-01   \n",
      "25%   -4.783748e-01 -4.747312e-01 -4.817743e-01 -4.785460e-01 -4.674504e-01   \n",
      "50%   -2.856022e-01 -2.861363e-01 -2.845076e-01 -2.862686e-01 -2.840696e-01   \n",
      "75%   -9.717587e-03 -2.199153e-02 -2.954564e-03 -1.290984e-02 -4.776187e-02   \n",
      "max    5.374187e+00  5.199826e+00  5.222207e+00  5.409591e+00  5.347656e+00   \n",
      "\n",
      "             Volume  \n",
      "count  2.056000e+03  \n",
      "mean   2.764758e-17  \n",
      "std    1.000243e+00  \n",
      "min   -1.067319e+00  \n",
      "25%   -5.556795e-01  \n",
      "50%   -2.660590e-01  \n",
      "75%    1.901122e-01  \n",
      "max    9.380783e+00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/will/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/will/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/will/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/will/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/will/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData types:\", data.dtypes)\n",
    "print(\"\\nMissing values:\\n\",data.isnull().sum())\n",
    "\n",
    "# select numeric columns for scaling\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "\n",
    "# initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale numeric features\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# describe the final dataset\n",
    "print(\"\\nFinal dataset overview:\")\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Shape: The dataset contains 2057 rows and 7 columns.\n",
    "\n",
    "Data Types: All columns except 'Date' are numeric (float64).\n",
    "\n",
    "Missing Values: There is one missing value in each column. We'll need to handle these missing values before proceeding with further analysis.\n",
    "\n",
    "Final Dataset Overview:\n",
    "\n",
    "Mean: The mean of each numeric column (except 'Volume') is approximately 0, indicating that the data has been centered around zero.\n",
    "Standard Deviation (std): The standard deviation of each numeric column is approximately 1, indicating that the data has been scaled to have unit variance.\n",
    "Min/Max: The minimum and maximum values of each column vary, but after scaling, they're within a comparable range.\n",
    "\n",
    "Interpretation of Scaling: By scaling the data, we've transformed the features such that they have mean 0 and standard deviation 1. This ensures that the features are on a similar scale, which can improve the performance of certain algorithms, such as logistic regression.\n",
    "Scaling helps in preventing features with larger magnitudes from dominating the optimization process and ensures that the model learns the underlying patterns more effectively.\n",
    "For instance, without scaling, the 'Volume' column, which has much larger values compared to the price columns, could disproportionately influence the model's predictions. Scaling mitigates this issue by putting all features on a comparable scale.\n",
    "In summary, scaling the data ensures that the logistic regression model can effectively learn from all features without being biased by differences in feature magnitudes. It helps in improving model convergence and performance.\n",
    "\n",
    "Handle Missing Values: Impute missing values with the mean of each numeric column, ensuring that the dataset is complete for analysis.\n",
    "\n",
    "Split the Data into Training and Testing Sets: Define features (X) and the target variable (y), then split the data into training and testing sets. Here, you can choose a relevant column as the target variable. Since we are dealing with stock price prediction, you might want to predict the future close price based on historical data. Therefore, you should keep the 'Close' column as the target variable.\n",
    "\n",
    "Discretize the Target Variable y: Since you want to predict whether to buy, sell, or hold stocks based on price movements, you need to discretize the 'Close' column into multiple classes. You can do this by categorizing the price movements into different categories (e.g., if the price increases, decreases, or remains the same). The number of bins will depend on how finely you want to categorize the price movements.\n",
    "\n",
    "Check Unique Values: After splitting the data and discretizing the target variable, it's essential to verify the unique values in both y_test (the actual target values in the testing set) and predictions (the predicted values). This check helps ensure that the model outputs and actual targets align properly and allows for further analysis of the classification results.\n",
    "\n",
    "So, the 'Close' column already exists in your dataset, and you'll use it as the target variable for predicting future price movements. Then, you'll discretize this column into multiple classes representing different price movements (e.g., buy, sell, hold) to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1645, 6) (1645,)\n",
      "Testing set shape: (412, 6) (412,)\n",
      "Unique values in y_test: [0 1 2]\n",
      "Unique values of y_bins: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# handle missing values\n",
    "# impute missing values using the mean of each numeric column (all of them)\n",
    "numeric_cols = data.select_dtypes(include=['float64']).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "X = data.drop(columns=['Date']) # dont need the date column for further processing\n",
    "y = data['Close']  # 'Close' variable is the target variable for prediction\n",
    "\n",
    "# discretize the target variable y\n",
    "# use the cut function with 4 bins\n",
    "y_bins = pd.cut(y, bins=3, labels=False)\n",
    "\n",
    "# split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bins, test_size=0.2, random_state=42)\n",
    "\n",
    "# display the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# check unique values in y_test\n",
    "print(\"Unique values in y_test:\", np.unique(y_test))\n",
    "# check unique values of y_bins\n",
    "print(\"Unique values of y_bins:\", np.unique(y_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set shape: (1645, 6) (1645,)\n",
    "\n",
    "This means that the training set contains 1645 samples (rows) and 6 features (columns). Additionally, there are 1645 target values corresponding to each sample. In other words, the training set consists of 1645 observations with 6 features each, along with their corresponding target values.\n",
    "Testing set shape: (412, 6) (412,)\n",
    "\n",
    "This means that the testing set contains 412 samples (rows) and 6 features (columns). Similarly, there are 412 target values corresponding to each sample. The testing set consists of 412 observations with 6 features each, along with their corresponding target values.\n",
    "In summary, the shape of the training and testing sets indicates the number of samples (observations) and features in each set, as well as the number of target values associated with each sample. These sets are ready for use in training and evaluating a machine learning model, such as a logistic regression model, for stock price prediction.\n",
    "\n",
    "We chose an 80/20 split for the following reasons:\n",
    "\n",
    "Balancing Training and Testing Data: An 80/20 split is a common choice that strikes a balance between having enough data for training the model effectively and having a sufficient amount of data for evaluating its performance. With 80% of the data allocated for training, the model can learn from a substantial portion of the dataset, while the 20% testing set provides a reasonable amount of data for evaluating the model's generalization performance.\n",
    "\n",
    "Trade-off between Bias and Variance: A larger training set (e.g., 80%) can help reduce bias by providing more data for the model to learn from, potentially leading to a more accurate model. However, a smaller testing set (e.g., 20%) can increase the variance of performance estimates, as there is less data available for evaluation. Nonetheless, in practice, an 80/20 split is often sufficient for obtaining reliable performance estimates while ensuring efficient use of data for training and testing.\n",
    "\n",
    "Computational Efficiency: Using a smaller testing set can lead to faster model evaluation, which is advantageous when experimenting with different models or hyperparameters.\n",
    "\n",
    "Overall, the 80/20 split is a widely used and practical choice for dividing data into training and testing sets, offering a good balance between model training and evaluation requirements. If you have specific considerations or requirements that suggest a different split ratio, feel free to adjust it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      count      mean       std       min       25%       50%       75%  \\\n",
      "Bin                                                                       \n",
      "0    1918.0 -0.246018  0.332366 -0.735688 -0.487813 -0.309435 -0.077774   \n",
      "1      67.0  2.592541  0.562413  1.440565  2.302837  2.624581  3.069873   \n",
      "2      72.0  4.141136  0.559406  3.381321  3.677325  4.019660  4.637407   \n",
      "\n",
      "          max  \n",
      "Bin            \n",
      "0    1.286129  \n",
      "1    3.345286  \n",
      "2    5.409591  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine 'Close' values with bin labels\n",
    "data_with_bins = pd.concat([y, y_bins], axis=1)\n",
    "data_with_bins.columns = ['Close', 'Bin']\n",
    "\n",
    "# Group data by bin labels\n",
    "grouped_data = data_with_bins.groupby('Bin')\n",
    "\n",
    "# Calculate summary statistics for 'Close' values within each bin\n",
    "summary_stats = grouped_data['Close'].describe()\n",
    "\n",
    "# Print summary statistics\n",
    "print(summary_stats)\n",
    "\n",
    "# Box plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "data_with_bins.boxplot(column='Close', by='Bin', grid=False)\n",
    "plt.title('Distribution of Close values within each bin')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Close')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin 0 (Low 'Close' values):\n",
    "\n",
    "Mean: Approximately -0.246\n",
    "Standard deviation (std): Approximately 0.332\n",
    "Interpretation: Bin 0 represents periods when the stock price is relatively low. This could be considered a **potential buying opportunity** ('BUY'), as the mean 'Close' value is below average and indicates a lower price range.\n",
    "\n",
    "Bin 1 (Moderate 'Close' values):\n",
    "\n",
    "Mean: Approximately 2.593\n",
    "Standard deviation (std): Approximately 0.562\n",
    "Interpretation: Bin 1 represents periods when the stock price is in a moderate range. This could suggest stability, and it may be advisable to **hold onto existing positions** ('HOLD') during these periods, as the mean 'Close' value is in a moderate range.\n",
    "\n",
    "Bin 2 (High 'Close' values):\n",
    "\n",
    "Mean: Approximately 4.141\n",
    "Standard deviation (std): Approximately 0.559\n",
    "Interpretation: Bin 2 represents periods when the stock price is relatively high. This could be considered a **potential selling opportunity** ('SELL'), as the mean 'Close' value is above average and indicates a higher price range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Modeling \n",
    "#### 2.1) Creating a Muilti-Class Classifier\n",
    "\n",
    "For creating a multi-classifier we must first create a seperate Binary Classifier for each unique class. We are tasked with creating a **One-Versus-All (OvA)** classifier where the samples belonging to the class being considered are treated as positive examples, while samples from other classes are treated as negative, for each seperate Binary Classifier trained. During the prediction stages we will run an instance through each Binary Classifier, choosing the one that has the highest probability.\n",
    "\n",
    "The following code defines a Binary Logistic Regression class that will be used to train and evaluate a multi-class classifier. It is derived from the *BinaryLogisticRegressionBase* class in eclarson's notebook *Logistic Regression.ipynb* and from the *BinaryLogisticRegression* class in his *Optimization.ipynb*. This includes a customizable regularization term C for when adjusted, affects the regularization term by regulating the trade-off between fitting the training data and keeping the model's weights small. A **higher value of C** implies less regularization, allowing the model to fit the training data more closely, potentially leading to overfitting. Conversely, a **lower value of C** increases the regularization strength, encouraging the model to generalize better to unseen data by keeping the weights smaller.  We are calculating gradient using a lower C value which is closer to **L2 reguralization** primarly to improve generalization by penalizing large coefficients, which in turn encourages the model to learn simpler patterns in the data. This is particularly important to our finacial prediction tasks, where the goal is often to generalize well to new market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Binary Logistic Regression class\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001): \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C # Adjustable C for regularization term (Using a lower C value to be closer to L2 reguralization)\n",
    "        # Internally store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if hasattr(self, 'w_'):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n' + str(self.w_)  # If we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    \n",
    "    # convenience, private\n",
    "    @staticmethod    \n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) # 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod  \n",
    "    def _add_bias(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X)) # add bias term\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Normilization\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    # public:   \n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing \n",
    "            \n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing Vectorization we can speed up our model. This is because python is not fast with for loops however, using vectorization in numpy is faster becasue numpy backend is written in C++. So this following class is just extending our BinaryLogisticRegression class to a Vectorized version. This code is coming from again eclarson's notebook *Logistic Regression.ipynb*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized coding\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_intercept=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "    \n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs  **Steepest Ascent Optimization** which is (also known as gradient ascent logistic regression) that is a method used to maximize the likelihood function of a logistic regression model by iteratively updating the model parameters (coefficients) in the direction of the gradient of the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZATION METHODS IN SEPERATE CLASSES\n",
    "#steepest ascent\n",
    "class SteepestAscentBinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C = 0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.w_ = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Steepest Ascent Binary Logistic Regression Object' if hasattr(self, 'w_') else 'Untrained Steepest Ascent Binary Logistic Regression Object'\n",
    "        \n",
    "    def _sigmoid(self, theta):\n",
    "        return expit(theta)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    def _get_gradient(self, X, y):\n",
    "        gradient = np.zeros(self.w_.shape)\n",
    "        for xi, yi in zip(X, y):\n",
    "            gradi = (yi - self.predict_proba(xi, add_intercept=False)) * xi\n",
    "            gradient += gradi.reshape(self.w_.shape)\n",
    "        \n",
    "        return gradient / float(len(y))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X)\n",
    "        self.w_ = np.zeros((Xb.shape[1], 1))\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta\n",
    "    \n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5)\n",
    "    \n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs **Stochastic Gradient Ascent Optimization (SGA)** and was derived from eclarson's notebook *Optimization.ipynb* that is similar to the previous binary classifiers but with changes in the get_gradient calculation to ensure that optimization technique is being performed. It is a variant of gradient ascent optimization where instead of computing the gradient of the loss function with respect to the model parameters using the entire dataset, the gradient is estimated using only a single randomly chosen sample at each iteration. This makes each update step much faster, allowing for faster convergence and scalability to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradienct ascent\n",
    "class StochasticGradientAscentBinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C = 0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.w_ = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Stochastic Gradient Ascent Binary Logistic Regression Object' if hasattr(self, 'w_') else 'Untrained Stochastic Gradient Binary Logistic Regression Object'\n",
    "        \n",
    "    def _sigmoid(self, theta):\n",
    "        return expit(theta)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        # grab a subset of samples in a mini-batch\n",
    "        # and calculate the gradient according to the small batch only\n",
    "        mini_batch_size = 16\n",
    "        idxs = np.random.choice(len(y), mini_batch_size)\n",
    "        \n",
    "        ydiff = y[idxs]-self.predict_proba(X[idxs],add_intercept=False).ravel() # get y difference (now scalar)\n",
    "        gradient = np.mean(X[idxs] * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C  # Use a positive sign here for gradient ascent\n",
    "\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X)\n",
    "        self.w_ = np.zeros((Xb.shape[1], 1))\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta\n",
    "    \n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5)\n",
    "    \n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs **Newton's Method of Optimization** and was derived from eclarson's notebook *Optimization.ipynb* that is similar to the previous binary classifiers but with changes in the get_gradient calculation to ensure that optimization technique is being performed. This method of optimization utilizes information about both the gradient (first derivative) and the curvature (second derivative) of the function to iteratively update the current solution in the direction of the minimum. Specifically, it computes the gradient and the Hessian matrix of the function at the current point and then solves a linear equation system to compute the Newton step, which represents the direction and magnitude of the update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "class NewtonsMethodBinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C = 0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.w_ = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Netwons Method Binary Logistic Regression Object' if hasattr(self, 'w_') else 'Untrained Netwons Method Binary Logistic Regression Object'\n",
    "        \n",
    "    def _sigmoid(self, theta):\n",
    "        return expit(theta)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X)\n",
    "        self.w_ = np.zeros((Xb.shape[1], 1))\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta\n",
    "    \n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5)\n",
    "\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_intercept=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our custom Multi-class Losgistical Regression classifier model that checks for a solver variable indicating what optimization technique to use and also what value of regularization term C to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our MultiClass Logistic Regression Classifier \n",
    "class MultiClassLogisticRegression:\n",
    "    # add in solver functionality \n",
    "    def __init__(self, eta=0.1, solver = \"steepest\", iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.solver = solver # Will default to Steepest Ascent\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        self.unique_ = None\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if hasattr(self, 'w_'):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n' + str(self.w_)  # If we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.unique_ = np.unique(y) # get each unique class value \n",
    "        for yval in self.unique_: # for each unique value\n",
    "            binary_y = (y == yval).astype(int) # create a binary problem \n",
    "            blr = VectorBinaryLogisticRegression(eta=self.eta, iterations=self.iters)\n",
    "            \n",
    "            # Checks for selected Optimization technique \n",
    "            if self.solver == \"steepest\":\n",
    "                blr = SteepestAscentBinaryLogisticRegression(self.eta, self.iters)\n",
    "            elif self.solver == 'stochastic':\n",
    "                blr = StochasticGradientAscentBinaryLogisticRegression(self.eta, self.iters)\n",
    "            elif self.solver == 'newton':\n",
    "                blr = NewtonsMethodBinaryLogisticRegression(self.eta, self.iters)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid solver. Choose from 'steepest', 'stochastic', or 'newton'.\") \n",
    "               \n",
    "            blr.fit(X, binary_y)\n",
    "            self.classifiers_.append(blr)\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape(-1, 1))\n",
    "        return np.hstack(probs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X), axis=1)]    \n",
    "\n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Training the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.68407631 -0.206199   -0.20691555 -0.20525336 -0.20610745 -0.20555559\n",
      "  -0.13508474]\n",
      " [-0.74896053  0.05854832  0.05884454  0.0592076   0.05913433  0.06071316\n",
      "   0.06567282]\n",
      " [-0.74631305  0.11166411  0.11171535  0.11071754  0.11128949  0.10963519\n",
      "   0.04504739]]\n",
      "Accuracy of  Steepest:  0.9514563106796117\n",
      "CPU times: user 600 ms, sys: 0 ns, total: 600 ms\n",
      "Wall time: 600 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the logistic regression model\n",
    "y_train = np.array(y_train)\n",
    "y_test= np.array(y_test)\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "\n",
    "# Steepest optimization\n",
    "mlr = MultiClassLogisticRegression(0.1,\"steepest\", iterations=20, C=0.001)\n",
    "mlr.fit(X_train,y_train)\n",
    "print(mlr)\n",
    "\n",
    "yhat = mlr.predict(X_test)\n",
    "print('Accuracy of  Steepest: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.67547605 -0.21752713 -0.21825584 -0.21578387 -0.21737889 -0.21843001\n",
      "  -0.13705382]\n",
      " [-0.7485238   0.06220191  0.06192141  0.06129769  0.06251998  0.06940117\n",
      "   0.07577613]\n",
      " [-0.73579614  0.12205356  0.1232708   0.11943711  0.12132696  0.11882462\n",
      "   0.02654866]]\n",
      "Accuracy of Stochastic:  0.9441747572815534\n",
      "CPU times: user 7.06 ms, sys: 309 s, total: 7.37 ms\n",
      "Wall time: 6.63 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Stochastic optimization\n",
    "mlr = MultiClassLogisticRegression(0.1,\"stochastic\", iterations=20, C=0.001)\n",
    "mlr.fit(X_train,y_train)\n",
    "print(mlr)\n",
    "\n",
    "yhat = mlr.predict(X_test)\n",
    "print('Accuracy of Stochastic: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 1.34668688 -0.17553361 -1.6314581   1.08645609 -0.3554137   0.42759085\n",
      "  -0.06458488]\n",
      " [-1.43346681 -1.1985089   1.53058773  1.65040043 -2.07408541  0.25926779\n",
      "   0.13966311]\n",
      " [-1.42924582  1.079577   -0.02653013 -2.23368935  2.16600519 -0.4982785\n",
      "  -0.07728812]]\n",
      "Accuracy of Newton:  0.9684466019417476\n",
      "CPU times: user 1.26 s, sys: 1.28 s, total: 2.54 s\n",
      "Wall time: 329 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Newtons's optimization\n",
    "X_train_10, _, y_train_10, _ = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
    "mlr = MultiClassLogisticRegression(0.1,\"newton\", iterations=10, C=0.001)\n",
    "mlr.fit(X_train_10,y_train_10)\n",
    "print(mlr)\n",
    "\n",
    "yhat = mlr.predict(X_test)\n",
    "print('Accuracy of Newton: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Comparing Our Classifiers Performance to SKL's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.60218398 -0.72941015 -0.77467328 -0.69198064 -0.77118997 -0.50339523\n",
      "  -0.47102669]\n",
      " [-4.17893124 -0.19797841 -0.63703954  0.53415124 -0.2251684   1.27734906\n",
      "   0.3875452 ]\n",
      " [-6.08121836  0.28182667  0.65799789  0.06381749  0.64641916  0.253786\n",
      "  -0.06476408]]\n",
      "Accuracy of:  0.970873786407767\n",
      "CPU times: user 55.3 ms, sys: 48 ms, total: 103 ms\n",
      "Wall time: 13.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X_train,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the scikit-learn model to our implementations are pretty much night and day. The scikit-learn model produced a slightly higher accuracy of **0.9708** compared to the Newton's method, which produced an accuracy of **0.9684**. However, the time difference is what really made the scikit-learn model more superior to our best model, as it classified our training and testing data in a CPU time of just **55.3 ms**, compared to **1.26 s**.  \n",
    "\n",
    "This time difference can be explained due paralellization, which is handled much better through scikit-learn as the operations are all performed in lower level C++. The accuracy can also be explained because the lr_sk object we created uses regularization, which controls over-learning. This optimization allows scikit-learn to process our data in much finer steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Deployment\n",
    "\n",
    "### 3.1) Implementation Suggestion\n",
    "\n",
    "For deployment of our model in real-world situations, we believe it would be best to use the **scikit-learn model**, as it is the most interpretable way to classify data. \n",
    "\n",
    "The scikit-learn model essentially compacts all of our previous logistic regression code and packages it in a library that can be utilized in just four lines of code. Scikit-learn's logistic regression implementation is optimized for processing datasets of various sizes. Even with our current dataset of 1645 training records and 412 testing records, scikit-learn's efficient processing ensures fast model training and prediction. Additionally, should our dataset grow larger in the future, scikit-learn's scalability ensures constant performance without compromising speed, like our custom models.\n",
    "\n",
    "Scikit-learn's widespread adoption into many different languages ensures access to further documentation should anything break in the future. With our custom implementation, detecting an error can be pearlous and can cost future developers or people maintaining our model a lot of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Exceptional Work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
