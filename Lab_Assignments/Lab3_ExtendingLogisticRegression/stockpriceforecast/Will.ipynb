{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2.970060</td>\n",
       "      <td>2.997060</td>\n",
       "      <td>2.902558</td>\n",
       "      <td>2.902558</td>\n",
       "      <td>2.349932</td>\n",
       "      <td>55057948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>2.920559</td>\n",
       "      <td>2.979060</td>\n",
       "      <td>2.916059</td>\n",
       "      <td>2.956560</td>\n",
       "      <td>2.393652</td>\n",
       "      <td>31801480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>3.006061</td>\n",
       "      <td>3.006061</td>\n",
       "      <td>2.965560</td>\n",
       "      <td>2.992560</td>\n",
       "      <td>2.422798</td>\n",
       "      <td>37002206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>2.988060</td>\n",
       "      <td>2.988060</td>\n",
       "      <td>2.929559</td>\n",
       "      <td>2.938559</td>\n",
       "      <td>2.379078</td>\n",
       "      <td>30158007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>2.866558</td>\n",
       "      <td>2.889058</td>\n",
       "      <td>2.857558</td>\n",
       "      <td>2.875558</td>\n",
       "      <td>2.328072</td>\n",
       "      <td>83954148.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close      Volume\n",
       "0  2015-02-17  2.970060  2.997060  2.902558  2.902558   2.349932  55057948.0\n",
       "1  2015-02-18  2.920559  2.979060  2.916059  2.956560   2.393652  31801480.0\n",
       "2  2015-02-19  3.006061  3.006061  2.965560  2.992560   2.422798  37002206.0\n",
       "3  2015-02-20  2.988060  2.988060  2.929559  2.938559   2.379078  30158007.0\n",
       "4  2015-02-23  2.866558  2.889058  2.857558  2.875558   2.328072  83954148.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# read the CSV file\n",
    "data = pd.read_csv('data/prices.csv')\n",
    "\n",
    "# display the first few rows of the dataframe\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the dataset primarily consists of stock prices and volume, scaling may not be strictly necessary, especially if interpretability is a concern. However, scaling could still be beneficial for ensuring that all features have similar ranges and for potentially improving convergence speed.\n",
    "\n",
    "Therefore, my recommendation would be to scale the data as a precautionary measure to ensure consistent behavior across different datasets and to potentially improve the model's performance. However, if interpretability is a top priority and you don't observe convergence issues during model training, you could choose to not scale the data.\n",
    "\n",
    "If you decide to proceed with scaling, I can include the scaling step in the preprocessing pipeline. Let me know your decision, and I'll adjust the code accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2057, 7)\n",
      "\n",
      "Data types:\n",
      "Date          object\n",
      "Open         float64\n",
      "High         float64\n",
      "Low          float64\n",
      "Close        float64\n",
      "Adj Close    float64\n",
      "Volume       float64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Date         0\n",
      "Open         1\n",
      "High         1\n",
      "Low          1\n",
      "Close        1\n",
      "Adj Close    1\n",
      "Volume       1\n",
      "dtype: int64\n",
      "\n",
      "Final dataset overview:\n",
      "               Open          High           Low         Close     Adj Close  \\\n",
      "count  2.056000e+03  2.056000e+03  2.056000e+03  2.056000e+03  2.056000e+03   \n",
      "mean   1.105903e-16 -1.658855e-16  1.105903e-16  2.211806e-16 -1.658855e-16   \n",
      "std    1.000243e+00  1.000243e+00  1.000243e+00  1.000243e+00  1.000243e+00   \n",
      "min   -7.369443e-01 -7.338389e-01 -7.615335e-01 -7.356884e-01 -6.718673e-01   \n",
      "25%   -4.783748e-01 -4.747312e-01 -4.817743e-01 -4.785460e-01 -4.674504e-01   \n",
      "50%   -2.856022e-01 -2.861363e-01 -2.845076e-01 -2.862686e-01 -2.840696e-01   \n",
      "75%   -9.717587e-03 -2.199153e-02 -2.954564e-03 -1.290984e-02 -4.776187e-02   \n",
      "max    5.374187e+00  5.199826e+00  5.222207e+00  5.409591e+00  5.347656e+00   \n",
      "\n",
      "             Volume  \n",
      "count  2.056000e+03  \n",
      "mean   2.764758e-17  \n",
      "std    1.000243e+00  \n",
      "min   -1.067319e+00  \n",
      "25%   -5.556795e-01  \n",
      "50%   -2.660590e-01  \n",
      "75%    1.901122e-01  \n",
      "max    9.380783e+00  \n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data.shape)\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# select numeric columns for scaling\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "\n",
    "# initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale numeric features\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# describe the final dataset\n",
    "print(\"\\nFinal dataset overview:\")\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Shape: The dataset contains 2057 rows and 7 columns.\n",
    "\n",
    "Data Types: All columns except 'Date' are numeric (float64).\n",
    "\n",
    "Missing Values: There is one missing value in each column. We'll need to handle these missing values before proceeding with further analysis.\n",
    "\n",
    "Final Dataset Overview:\n",
    "\n",
    "Mean: The mean of each numeric column (except 'Volume') is approximately 0, indicating that the data has been centered around zero.\n",
    "Standard Deviation (std): The standard deviation of each numeric column is approximately 1, indicating that the data has been scaled to have unit variance.\n",
    "Min/Max: The minimum and maximum values of each column vary, but after scaling, they're within a comparable range.\n",
    "Interpretation of Scaling:\n",
    "\n",
    "By scaling the data, we've transformed the features such that they have mean 0 and standard deviation 1. This ensures that the features are on a similar scale, which can improve the performance of certain algorithms, such as logistic regression.\n",
    "Scaling helps in preventing features with larger magnitudes from dominating the optimization process and ensures that the model learns the underlying patterns more effectively.\n",
    "For instance, without scaling, the 'Volume' column, which has much larger values compared to the price columns, could disproportionately influence the model's predictions. Scaling mitigates this issue by putting all features on a comparable scale.\n",
    "In summary, scaling the data ensures that the logistic regression model can effectively learn from all features without being biased by differences in feature magnitudes. It helps in improving model convergence and performance.\n",
    "\n",
    "Handle Missing Values: Impute missing values with the mean of each numeric column, ensuring that the dataset is complete for analysis.\n",
    "\n",
    "Split the Data into Training and Testing Sets: Define features (X) and the target variable (y), then split the data into training and testing sets. Here, you can choose a relevant column as the target variable. Since we are dealing with stock price prediction, you might want to predict the future close price based on historical data. Therefore, you should keep the 'Close' column as the target variable.\n",
    "\n",
    "Discretize the Target Variable y: Since you want to predict whether to buy, sell, or hold stocks based on price movements, you need to discretize the 'Close' column into multiple classes. You can do this by categorizing the price movements into different categories (e.g., if the price increases, decreases, or remains the same). The number of bins will depend on how finely you want to categorize the price movements.\n",
    "\n",
    "Check Unique Values: After splitting the data and discretizing the target variable, it's essential to verify the unique values in both y_test (the actual target values in the testing set) and predictions (the predicted values). This check helps ensure that the model outputs and actual targets align properly and allows for further analysis of the classification results.\n",
    "\n",
    "So, the 'Close' column already exists in your dataset, and you'll use it as the target variable for predicting future price movements. Then, you'll discretize this column into multiple classes representing different price movements (e.g., buy, sell, hold) to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1645, 6) (1645,)\n",
      "Testing set shape: (412, 6) (412,)\n",
      "Unique values in y_test: [0 1 2 3]\n",
      "Unique values of y_bins: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# handle missing values\n",
    "# impute missing values using the mean of each numeric column (all of them)\n",
    "numeric_cols = data.select_dtypes(include=['float64']).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "X = data.drop(columns=['Date']) # dont need the date column for further processing\n",
    "y = data['Close']  # 'Close' variable is the target variable for prediction\n",
    "\n",
    "# discretize the target variable y\n",
    "# use the cut function with 4 bins\n",
    "y_bins = pd.cut(y, bins=4, labels=False)\n",
    "\n",
    "# split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bins, test_size=0.2, random_state=42)\n",
    "\n",
    "# display the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# check unique values in y_test\n",
    "print(\"Unique values in y_test:\", np.unique(y_test))\n",
    "# check unique values of y_bins\n",
    "print(\"Unique values of y_bins:\", np.unique(y_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set shape: (1645, 6) (1645,)\n",
    "\n",
    "This means that the training set contains 1645 samples (rows) and 6 features (columns). Additionally, there are 1645 target values corresponding to each sample. In other words, the training set consists of 1645 observations with 6 features each, along with their corresponding target values.\n",
    "Testing set shape: (412, 6) (412,)\n",
    "\n",
    "This means that the testing set contains 412 samples (rows) and 6 features (columns). Similarly, there are 412 target values corresponding to each sample. The testing set consists of 412 observations with 6 features each, along with their corresponding target values.\n",
    "In summary, the shape of the training and testing sets indicates the number of samples (observations) and features in each set, as well as the number of target values associated with each sample. These sets are ready for use in training and evaluating a machine learning model, such as a logistic regression model, for stock price prediction.\n",
    "\n",
    "We chose an 80/20 split for the following reasons:\n",
    "\n",
    "Balancing Training and Testing Data: An 80/20 split is a common choice that strikes a balance between having enough data for training the model effectively and having a sufficient amount of data for evaluating its performance. With 80% of the data allocated for training, the model can learn from a substantial portion of the dataset, while the 20% testing set provides a reasonable amount of data for evaluating the model's generalization performance.\n",
    "\n",
    "Trade-off between Bias and Variance: A larger training set (e.g., 80%) can help reduce bias by providing more data for the model to learn from, potentially leading to a more accurate model. However, a smaller testing set (e.g., 20%) can increase the variance of performance estimates, as there is less data available for evaluation. Nonetheless, in practice, an 80/20 split is often sufficient for obtaining reliable performance estimates while ensuring efficient use of data for training and testing.\n",
    "\n",
    "Computational Efficiency: Using a smaller testing set can lead to faster model evaluation, which is advantageous when experimenting with different models or hyperparameters.\n",
    "\n",
    "Overall, the 80/20 split is a widely used and practical choice for dividing data into training and testing sets, offering a good balance between model training and evaluation requirements. If you have specific considerations or requirements that suggest a different split ratio, feel free to adjust it accordingly.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9514563106796117\n"
     ]
    }
   ],
   "source": [
    "# define the Binary Logistic Regression class\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # Internally store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if hasattr(self, 'w_'):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n' + str(self.w_)  # If we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _sigmoid(self, theta):\n",
    "        return 1 / (1 + np.exp(-theta))\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    def _get_gradient(self, X, y):\n",
    "        gradient = np.zeros(self.w_.shape)\n",
    "        for (xi, yi) in zip(X, y):\n",
    "            gradi = (yi - self._sigmoid(xi @ self.w_)) * xi \n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient / float(len(y))\n",
    "       \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        Xb = self._add_intercept(X)\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "# define the MultiClass Logistic Regression class\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta=0.1, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.classifiers_ = []\n",
    "        self.unique_ = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        if hasattr(self, 'w_'):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n' + str(self.w_)  # If we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.unique_ = np.unique(y)\n",
    "        for yval in self.unique_:\n",
    "            binary_y = (y == yval).astype(int)\n",
    "            blr = BinaryLogisticRegression(eta=self.eta, iterations=self.iters)\n",
    "            blr.fit(X, binary_y)\n",
    "            self.classifiers_.append(blr)\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape(-1, 1))\n",
    "        return np.hstack(probs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X), axis=1)]\n",
    "    \n",
    "# train the logistic regression model\n",
    "mlr = MultiClassLogisticRegression()\n",
    "mlr.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the trained model using y_bins instead of y_test\n",
    "predictions = mlr.predict(X_test)\n",
    "# calculate accuracy using y_bins instead of y_test\n",
    "print('Accuracy:', accuracy_score(y_bins[X_test.index], predictions))\n",
    "\n",
    "# All credit to instructor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 1: Stock Price: -0.38356534094497124 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 2: Stock Price: 2.881975321865618 | Predicted Action: HOLD ONTO THE STOCK | Actual Action: HOLD ONTO THE STOCK | Prediction: Correct\n",
      "Instance 3: Stock Price: -0.5851094679218563 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 4: Stock Price: -0.4854956803452672 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 5: Stock Price: -0.02680915304393465 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 6: Stock Price: -0.37198258182643495 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 7: Stock Price: 2.9797852877554796 | Predicted Action: HOLD ONTO THE STOCK | Actual Action: HOLD ONTO THE STOCK | Prediction: Correct\n",
      "Instance 8: Stock Price: -0.4391641290818278 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 9: Stock Price: 0.05427170515370183 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n",
      "Instance 10: Stock Price: -0.5503606757769531 | Predicted Action: BUY THE STOCK | Actual Action: BUY THE STOCK | Prediction: Correct\n"
     ]
    }
   ],
   "source": [
    "# define actions based on predicted labels\n",
    "actions = {0: \"BUY THE STOCK\", 1: \"SELL THE STOCK\", 2: \"HOLD ONTO THE STOCK\", 3: \"NO ACTION\"}\n",
    "\n",
    "# print interpretations of predictions along with stock prices and correctness\n",
    "for idx, (actual, predicted) in enumerate(zip(y_test[:10], predictions[:10])):\n",
    "    stock_price = X_test.iloc[idx]['Close']  # Assuming 'Close' column represents the stock price\n",
    "    correct = \"Correct\" if actual == predicted else \"Incorrect\"\n",
    "    print(f\"Instance {idx + 1}: Stock Price: {stock_price} | Predicted Action: {actions[predicted]} | Actual Action: {actions[actual]} | Prediction: {correct}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
