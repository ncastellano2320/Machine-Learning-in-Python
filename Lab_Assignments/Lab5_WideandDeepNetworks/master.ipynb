{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 5: Wide and Deep Networks**\n",
    "### Authors: Will Lahners, Edward Powers, and Nino Castellano\n",
    "\n",
    "## **Describing the Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset we chose pertains to mushrooms, specifically whether or not they are poisonous, and is called *mushrooms.csv*. We obtained this data from [Kaggle](https://www.kaggle.com/datasets/uciml/mushroom-classification). The results of our model could benefit food production companies, farmers, or people who enjoy the outdoors. \n",
    "\n",
    "We chose this dataset becasue every feature column is a categorical variable, which makes this perfect for a wide and deep neural network. This data set contains descriptions of samples corresponding to 23 species of gilled mushrooms found in the Agaricus and Lepiota Family Mushroom. Each species will be identified to be either definitely edible, definitely poisonous, or unknown edibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (4 points total)\n",
    "\n",
    "> [1 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). You have the option of using tf.dataset for processing, but it is not required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edible</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>stalk-root</th>\n",
       "      <th>stalk-surface-above-ring</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   edible  cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
       "0   False          5            2          4        1     6                1   \n",
       "1    True          5            2          9        1     0                1   \n",
       "2    True          0            2          8        1     3                1   \n",
       "3   False          5            3          8        1     6                1   \n",
       "4    True          5            2          3        0     5                1   \n",
       "\n",
       "   gill-spacing  gill-size  gill-color  stalk-shape  stalk-root  \\\n",
       "0             0          1           4            0           3   \n",
       "1             0          0           4            0           2   \n",
       "2             0          0           5            0           2   \n",
       "3             0          1           5            0           3   \n",
       "4             1          0           4            1           3   \n",
       "\n",
       "   stalk-surface-above-ring  stalk-surface-below-ring  stalk-color-above-ring  \\\n",
       "0                         2                         2                       7   \n",
       "1                         2                         2                       7   \n",
       "2                         2                         2                       7   \n",
       "3                         2                         2                       7   \n",
       "4                         2                         2                       7   \n",
       "\n",
       "   stalk-color-below-ring  veil-color  ring-number  ring-type  \\\n",
       "0                       7           2            1          4   \n",
       "1                       7           2            1          4   \n",
       "2                       7           2            1          4   \n",
       "3                       7           2            1          4   \n",
       "4                       7           2            1          0   \n",
       "\n",
       "   spore-print-color  population  habitat  \n",
       "0                  2           3        5  \n",
       "1                  3           2        1  \n",
       "2                  3           2        3  \n",
       "3                  2           3        5  \n",
       "4                  3           0        1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# (1) Load the data into a pandas DataFrame\n",
    "data = pd.read_csv('./mushrooms.csv')\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Deleting Useless Variables\n",
    "data.drop(columns=['veil-type'], inplace=True)\n",
    "\n",
    "class_mapping = {'e': True, 'p': False}\n",
    "data['class'] = data['class'].map(class_mapping)\n",
    "\n",
    "# Optionally, rename the column to something more descriptive\n",
    "data.rename(columns={'class': 'edible'}, inplace=True)\n",
    "\n",
    "# Encode any string data as integers for now (Credits to ChatGPT)\n",
    "le = LabelEncoder()\n",
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in object_columns:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    \n",
    "pd.set_option('display.max_columns', None)    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 22 columns):\n",
      " #   Column                    Non-Null Count  Dtype\n",
      "---  ------                    --------------  -----\n",
      " 0   edible                    8124 non-null   bool \n",
      " 1   cap-shape                 8124 non-null   int64\n",
      " 2   cap-surface               8124 non-null   int64\n",
      " 3   cap-color                 8124 non-null   int64\n",
      " 4   bruises                   8124 non-null   int64\n",
      " 5   odor                      8124 non-null   int64\n",
      " 6   gill-attachment           8124 non-null   int64\n",
      " 7   gill-spacing              8124 non-null   int64\n",
      " 8   gill-size                 8124 non-null   int64\n",
      " 9   gill-color                8124 non-null   int64\n",
      " 10  stalk-shape               8124 non-null   int64\n",
      " 11  stalk-root                8124 non-null   int64\n",
      " 12  stalk-surface-above-ring  8124 non-null   int64\n",
      " 13  stalk-surface-below-ring  8124 non-null   int64\n",
      " 14  stalk-color-above-ring    8124 non-null   int64\n",
      " 15  stalk-color-below-ring    8124 non-null   int64\n",
      " 16  veil-color                8124 non-null   int64\n",
      " 17  ring-number               8124 non-null   int64\n",
      " 18  ring-type                 8124 non-null   int64\n",
      " 19  spore-print-color         8124 non-null   int64\n",
      " 20  population                8124 non-null   int64\n",
      " 21  habitat                   8124 non-null   int64\n",
      "dtypes: bool(1), int64(21)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by one-hot encoding the each of our variables. We also ended up dropping the 'veil-type' becasue that column did not contain any unique values that would benefit our model. Each column is categorical, with each number (being one-hot encoded) pertaining to a differnet feature of that categorical variable. In the table below, the features of our variables can be found:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Number of Classifications | Types of Classifications |\n",
    "|:-------------|:--------------:|--------------:|\n",
    "| edible        |       2    |           true or false  |\n",
    "| cap-shape         |       6       |          bell, conical, flat, sunken, convex, or knobbed    |\n",
    "| cap-surface        |     4       |        grooves, scaley, smooth, or fiberous   |\n",
    "| cap-color        |     10       |        gray, green, brown, buff, cinamon, pink, purple, red, white, or yellow   |\n",
    "| bruises        |     2       |        true or false   |\n",
    "| odor        |     9       |        fishy, foul, musty, pungent, spicy, anise, creosote, almond, or none   |\n",
    "| gill-attachment        |     4       |        descending, free, attached, or notched   |\n",
    "| gill-spacing        |     3       |        close, distant, or crowded   |\n",
    "| gill-size        |     2       |        broad or narrow   |\n",
    "| gill-color        |     12       |        white, black, brown, chocolate, gray, buff, green, yellow, orange, pink, purple or red   |\n",
    "| stalk-shape        |     2       |        enlarging or taperingg   |\n",
    "| stalk-root        |     7       |        club, cup, equal, rhizomorphs, missing, bulbous, or rooted.   |\n",
    "| stalk-surface-above-ring        |     4       |        scaly, silky, smooth, or fiberous   |\n",
    "| stalk-surface-below-ring        |     4       |        scaly, silky, smooth, or fiberous   |\n",
    "| stalk-color-above-ring        |     8       |        gray, cinnamon, orange, pink, red, yellow, buff, or brown   |\n",
    "| stalk-color-below-ring        |     8       |        gray, cinnamon, orange, pink, red, yellow, buff, or brown   |\n",
    "| veil-type        |     2       |       partial or universal   |\n",
    "| veil-color        |     4       |        brown, orange, yellow, or white   |\n",
    "| ring-number        |     3       |        none, one, or two   |\n",
    "| ring-type        |     8       |        cobwebby, evanscent, flaring, large, none, pendant, sheating, or zone   |\n",
    "| spore-print-color        |     9       |        brown, buff, black, green, orange, purple, white, yellow, or chocolate  |\n",
    "| population        |     6       |        clustered, numerous, scattered, several, solitary, or abundant    |\n",
    "| habitat        |     7       |        grasses, meadows, leaves, paths, urban, woods or waste   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1 points] Identify groups of features in your data that should be combined into cross-product features. Provide a compelling justification for why these features should be crossed (or why some features should not be crossed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import FeatureSpace\n",
    "\n",
    "#Creating Feature Spaces\n",
    "feature_space= FeatureSpace(\n",
    "    features= {\n",
    "        \"cap-shape\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"cap-surface\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"cap-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"bruises\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"odor\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-attachment\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-spacing\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-size\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-shape\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-root\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-surface-above-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-surface-below-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-color-above-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-color-below-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"veil-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"ring-number\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"ring-type\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"spore-print-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"population\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"habitat\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "    }, crosses=[\n",
    "        # Cap-Color and Cap Shape\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('cap-color', 'cap-shape'),\n",
    "            crossing_dim= 10*6),\n",
    "        # Odor and Gill-Color\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('gill-color', 'odor'),\n",
    "            crossing_dim= 12*9),\n",
    "        # Bruises and Stalk-Surface-above-ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('bruises', 'stalk-surface-above-ring'),\n",
    "            crossing_dim= 2*4),\n",
    "        # Bruises and Stalk-Surface-below-ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('bruises', 'stalk-surface-below-ring'),\n",
    "            crossing_dim= 2*4),\n",
    "        # Ring-Type and Stalk-Color-below ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('ring-type', 'stalk-color-below-ring'),\n",
    "            crossing_dim= 8*8),\n",
    "        # Ring-Type and Stalk-Color-above ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('ring-type', 'stalk-color-above-ring'),\n",
    "            crossing_dim= 8*8),\n",
    "        # Spore-Print Color and Habitat\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('spore-print-color', 'habitat'),\n",
    "            crossing_dim= 9*7)\n",
    "    ],\n",
    "    output_mode=\"concat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we establish the feature space utilized by our network. Initially, we inform Keras that all integer values within the dataframe denote categorical values. This specification will prove advantageous when we proceed to one-hot encode this data in the subsequent section.\n",
    "\n",
    "After conducting preliminary research into various qualities and characteristics commonly associated with poisonous mushrooms, we found that we could use some of our features from our dataset to combine them into cross-product features possibly improving the predictive performance of the model in distinguishing between edible and poisonous mushrooms.\n",
    "\n",
    "We found we combine the following features into the feature space:\n",
    "\n",
    "- **Cap-Color X Cap-Shape**: Certain combinations of cap color and shape might be more indicative of edible or poisonous mushrooms. For example, convex-shaped mushrooms with a brown cap color might be more likely to be edible, while flat-shaped mushrooms with a red cap color might be more likely to be poisonous.\n",
    "\n",
    "- **Odor X Gill-Color**: The combination of odor and gill color can provide valuable information. For instance, mushrooms with a foul odor and black gills might be more likely to be poisonous, while mushrooms with an almond-like odor and white gills might be more likely to be edible.\n",
    "\n",
    "- **Bruises X Stalk-Surface**: Combining bruises and stalk surface texture could capture interactions related to the mushroom's response to damage. For example, mushrooms that bruise easily and have a silky stalk surface might be more likely to be poisonous.\n",
    "\n",
    "- **Ring-Type X Stalk-Color**: Certain combinations of ring type and stalk color might be indicative of edible or poisonous mushrooms. For instance, mushrooms with an evanescent ring type and a brown stalk color might be more likely to be edible.\n",
    "\n",
    "- **Spore-Print-Color X Habitat**: Combining spore print color and habitat could capture interactions related to the mushroom's reproductive characteristics and preferred environment. For example, mushrooms with a brown spore print color found in wooded habitats might be more likely to be edible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin modeling, we must ensure what evaluation metrics are appropriate for evaluating our networks performance, as it pertains to our buisness case. For evaluating our network's performance on classifying mushrooms as edible or poisonous, using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) is a prudent choice, particularly due to the critical nature of the classification task.\n",
    "\n",
    "The primary concern in classifying mushrooms is the potential severe health risks associated with incorrectly identifying a poisonous mushroom as edible. In this context, the consequences of false negatives (wrongly predicting that a poisonous mushroom is edible) are far more severe than false positives (erroneously identifying an edible mushroom as poisonous). The AUC-ROC metric provides a comprehensive measure of the model’s ability to correctly classify both classes across all possible thresholds, emphasizing the capability to distinguish between the two with high sensitivity (true positive rate) and specificity (true negative rate).\n",
    "\n",
    "Since our primary goal is to avoid false negatives, the ROC curve (which plots the true positive rate against the false positive rate) helps in visualizing and choosing a model with the least number of false negatives at an acceptable false positive rate level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Argue why your cross validation method is a realistic mirroring of how an algorithm would be used in practice. Use the method to split your data that you argue for. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in shuffle_split.split(data):\n",
    "  df_train= data.iloc[train_index]\n",
    "  df_test= data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To divide our dataset for training and testing purposes, we will employ the shuffle split method from the model_selection package in Scikit-Learn. This method is particularly suitable for our dataset given its relatively balanced composition, with 52% of the observations being edible and 48% poisonous. The even distribution facilitates the use of the shuffle split method, which is not only faster but also less computationally demanding. We have opted for an 80-20 split between the training and testing sets, respectively. This ratio helps minimize the risk of overfitting while ensuring that the testing set remains adequately large to verify the model's performance effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors for train and test datasets\n",
    "categorical_headers= data.drop(columns=['edible']).columns\n",
    "batch_size= 64\n",
    "\n",
    "def create_dataset_from_dataframe(df_input):\n",
    "\n",
    "    df = df_input.copy()\n",
    "    labels = df_input['edible'].values\n",
    "    \n",
    "    # Removing labels from the features data\n",
    "    df = df.drop(columns=['edible'])\n",
    "\n",
    "    df = {key: value.values[:,np.newaxis] for key, value in df_input[categorical_headers].items()}\n",
    "\n",
    "    # create the Dataset here\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "\n",
    "    # now enable batching and prefetching\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "\n",
    "    return ds\n",
    "\n",
    "ds_train= create_dataset_from_dataframe(df_train)\n",
    "ds_test= create_dataset_from_dataframe(df_test)\n",
    "\n",
    "# Performing One Hot Encoding\n",
    "ds_train_no_label= ds_train.map(lambda x, _: x)\n",
    "feature_space.adapt(ds_train_no_label)\n",
    "\n",
    "train_ds_with_no_labels = ds_train.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_with_no_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we start by converting our Pandas DataFrame into a TensorFlow Tensor. This conversion is facilitated by a function taken from example 10a on our class's GitHub repository. Once we have transformed the train and test datasets into tensors, we can straightforwardly apply one-hot encoding using the Keras Feature Space object that was established in the preceding section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (5 points total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [2 points] Create at least three combined wide and deep networks to classify your data using Keras (this total of \"three\" includes the model you will train in the next step of the rubric). Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations.\n",
    "\n",
    "> *Note: you can use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "102/102 - 6s - loss: 0.5076 - auc: 0.9627 - val_loss: 0.2733 - val_auc: 0.9973 - 6s/epoch - 55ms/step\n",
      "Epoch 2/5\n",
      "102/102 - 1s - loss: 0.1287 - auc: 0.9989 - val_loss: 0.0495 - val_auc: 0.9999 - 532ms/epoch - 5ms/step\n",
      "Epoch 3/5\n",
      "102/102 - 1s - loss: 0.0303 - auc: 0.9996 - val_loss: 0.0178 - val_auc: 0.9999 - 683ms/epoch - 7ms/step\n",
      "Epoch 4/5\n",
      "102/102 - 1s - loss: 0.0151 - auc: 0.9999 - val_loss: 0.0104 - val_auc: 0.9999 - 606ms/epoch - 6ms/step\n",
      "Epoch 5/5\n",
      "102/102 - 1s - loss: 0.0106 - auc: 0.9999 - val_loss: 0.0074 - val_auc: 1.0000 - 621ms/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import FeatureSpace\n",
    "from tensorflow.keras.layers import Dense, Embedding, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def embedding_from_crossing(feature_space, col_name):\n",
    "    N = feature_space.crossers[col_name].num_bins\n",
    "    x = feature_space.crossers[col_name].output\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1, name=col_name+'_embed')(x)\n",
    "    x = Flatten()(x)\n",
    "    return x\n",
    "\n",
    "def embedding_from_categorical(feature_space, col_name):\n",
    "    N = len(feature_space.preprocessors[col_name].get_vocabulary())\n",
    "    x = feature_space.preprocessors[col_name].output\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1, name=col_name+'_embed')(x)\n",
    "    x = Flatten()(x)\n",
    "    return x\n",
    "\n",
    "# Model Definition\n",
    "def create_wide_and_deep_model(feature_space):\n",
    "    dict_inputs = feature_space.get_inputs()\n",
    "\n",
    "    # Wide Branch\n",
    "    crossed_outputs = [embedding_from_crossing(feature_space, col) for col in feature_space.crossers.keys()]\n",
    "    wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "    # Deep Branch\n",
    "    deep_branch_outputs = [embedding_from_categorical(feature_space, col) for col in categorical_headers]\n",
    "    deep_branch = Concatenate(name='embed_concat')(deep_branch_outputs)\n",
    "    deep_branch = Dense(units=20, activation='relu', name='deep_1')(deep_branch)\n",
    "    deep_branch = Dense(units=5, activation='relu', name='deep_2')(deep_branch)\n",
    "\n",
    "    # Combined Branch\n",
    "    final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "    final_branch = Dense(units=1, activation='sigmoid', name='combined')(final_branch)\n",
    "\n",
    "    model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Create and Train Model\n",
    "model = create_wide_and_deep_model(feature_space)\n",
    "history = model.fit(ds_train, epochs=5, validation_data=ds_test, verbose=2)\n",
    "\n",
    "# Credit to instructor code / Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 5ms/step - loss: 0.0074 - auc: 1.0000\n",
      "Loss: 0.007446132134646177\n",
      "Accuracy: 0.9999529123306274\n"
     ]
    }
   ],
   "source": [
    "# Visualization onto one plot for simplicity\n",
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.plot(history.history['auc'], label='Training AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss/AUC')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Model Evaluation\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our model's outputs, let's interpret the graph. The loss curves, specifically the training loss curve, shows us how the model's loss on the training data decreases over epochs. A lower loss indicates better performance on the training data, as we can see in this graph, the training loss starts high are drops quickly, which is a good indication that our model is learning efficiently. Now for the validation loss, this curve represents the model's loss on validation data, which is the data not used for training, over our 5 epochs. It helps to assess how well the mdoel responds to unseen data. This curve is relatively concerning, as it could present the possibilty the our model may be slightly overfitting to the training data, as it should have a somewhat similar curve to the training loss. \n",
    "\n",
    "We can also look at the training AUC, which measures the model's ability to tell between positive and negative classes, and just after 2 epochs, the training AUC flattens at 1, which indicates that the model is learning how to classify the mushrooms very well based on our training data. For the validation AUC, we are seeing some a-typical results as it seems to flatten at 1 on the first epoch. This could also hint at our model being overfitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [2 points] Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two models (this \"two\" includes the wide and deep model trained from the previous step). Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to answer: What model with what number of layers performs superiorly? Use proper statistical methods to compare the performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wide_and_deep_model_with_depth(feature_space, deep_units):\n",
    "    dict_inputs = feature_space.get_inputs()\n",
    "\n",
    "    # Wide Branch (Same as before)\n",
    "    crossed_outputs = [embedding_from_crossing(feature_space, col) for col in feature_space.crossers.keys()]\n",
    "    wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "    # Deep Branch\n",
    "    deep_branch_outputs = [embedding_from_categorical(feature_space, col) for col in categorical_headers]\n",
    "    deep_branch = Concatenate(name='embed_concat')(deep_branch_outputs)\n",
    "    for units in deep_units:\n",
    "        deep_branch = Dense(units=units, activation='relu')(deep_branch)\n",
    "\n",
    "    # Combined Branch (Same as before)\n",
    "    final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "    final_branch = Dense(units=1, activation='sigmoid', name='combined')(final_branch)\n",
    "\n",
    "    model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Create Models\n",
    "model1 = create_wide_and_deep_model_with_depth(feature_space, deep_units=[20, 5])  # Base model\n",
    "model2 = create_wide_and_deep_model_with_depth(feature_space, deep_units=[30, 20, 10, 5])  # Deeper model\n",
    "model3 = create_wide_and_deep_model_with_depth(feature_space, deep_units=[10])  # Shallower model\n",
    "\n",
    "# Credit to Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code cell, we create 3 models of varying depth. model1 creates the base model with two layers in the deep branch (20 units and 5 units), model2 creates a deeper model with four layers in the deep branch (30, 20, 10, and units), and model3 creates a shallower model with only one layer in the deep branch (10 units). Let's now proceed to some cross-validation testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 1s 4ms/step\n",
      "26/26 [==============================] - 1s 4ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 6ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 5ms/step\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 5ms/step\n",
      "Model 1 AUC (Base): 0.9999960566866462\n",
      "Model 2 AUC (Deeper): 0.9999805867116563\n",
      "Model 3 AUC (Shallower): 0.9999884733917348\n",
      "p-value (Model 1 vs. Model 2): 0.352676495827291\n",
      "p-value (Model 1 vs. Model 3): 0.373900966300059\n",
      "p-value (Model 2 vs. Model 3): 0.3320568503669345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import stats\n",
    "\n",
    "# Cross-Validation (k = 5)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results_model1 = []\n",
    "cv_results_model2 = []\n",
    "cv_results_model3 = []\n",
    "\n",
    "for train_index, val_index in cv.split(data.drop('edible', axis=1), data['edible']):\n",
    "    X_train, X_val = data.drop('edible', axis=1).iloc[train_index], data.drop('edible', axis=1).iloc[val_index]\n",
    "    y_train, y_val = data['edible'].iloc[train_index], data['edible'].iloc[val_index]\n",
    "\n",
    "    # Convert data to TensorFlow datasets\n",
    "    ds_train = create_dataset_from_dataframe(pd.concat([X_train, y_train], axis=1))\n",
    "    ds_val = create_dataset_from_dataframe(pd.concat([X_val, y_val], axis=1))\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    model1.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
    "    y_pred_proba_model1 = model1.predict(ds_val).flatten()\n",
    "    auc_model1 = roc_auc_score(y_val, y_pred_proba_model1)\n",
    "    cv_results_model1.append(auc_model1)\n",
    "\n",
    "    # Model 2 (Deeper)\n",
    "    model2.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
    "    y_pred_proba_model2 = model2.predict(ds_val).flatten()\n",
    "    auc_model2 = roc_auc_score(y_val, y_pred_proba_model2)\n",
    "    cv_results_model2.append(auc_model2)\n",
    "\n",
    "    # Model 3 (Shallower)\n",
    "    model3.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
    "    y_pred_proba_model3 = model3.predict(ds_val).flatten()\n",
    "    auc_model3 = roc_auc_score(y_val, y_pred_proba_model3)\n",
    "    cv_results_model3.append(auc_model3)\n",
    "\n",
    "# Statistical Comparison (Paired T-test)\n",
    "_, p_value_1_2 = stats.ttest_rel(cv_results_model1, cv_results_model2)\n",
    "_, p_value_1_3 = stats.ttest_rel(cv_results_model1, cv_results_model3)\n",
    "_, p_value_2_3 = stats.ttest_rel(cv_results_model2, cv_results_model3)\n",
    "\n",
    "# Print Results\n",
    "print(\"Model 1 AUC (Base):\", np.mean(cv_results_model1))\n",
    "print(\"Model 2 AUC (Deeper):\", np.mean(cv_results_model2))\n",
    "print(\"Model 3 AUC (Shallower):\", np.mean(cv_results_model3))\n",
    "print(\"p-value (Model 1 vs. Model 2):\", p_value_1_2)\n",
    "print(\"p-value (Model 1 vs. Model 3):\", p_value_1_3)\n",
    "print(\"p-value (Model 2 vs. Model 3):\", p_value_2_3)\n",
    "\n",
    "# Credit to Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, we can make several conclustions. \n",
    "\n",
    "As we can see from the base model (model1), it achieved a perfect average AUC of **1**, which indicates perfect performance in distinguishing edible versus poisonous mushrooms. The deeper model, model2, acheived an accuracy of **0.99**, which is also very close to perfect performance. And lastly, model3, the shallower model, acheived an also near-perfect performance of **0.99**. **Because all of the p-values are greater than a 95% confience interval, we can conclude that there is no statistically significant different in performance between any pairs of models we created.**\n",
    "\n",
    "There is no evidence to suggest that increasing or decreasing the depth of the deep branch leads to a statistically significant improvment or reduction in the generalization performance. However, we can also say that the simpler model (shallower), could be preferred as it performed very similarly to the other two models, with only one layer in the deep branch. **This simpler model is lightweight, and is much less computationally expensive than the other two, so we would prefer it for a business case.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a deep-only model that will be structued the same as model 3 (the shallow model), but without using the wide branch. It will only use the embeddings of the categoricals passed through a single dense layer of 10 unites and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_only_model(feature_space):\n",
    "    dict_inputs = feature_space.get_inputs()\n",
    "\n",
    "    # Deep Branch\n",
    "    deep_branch_outputs = [embedding_from_categorical(feature_space, col) for col in categorical_headers]\n",
    "    deep_branch = Concatenate(name='embed_concat')(deep_branch_outputs)\n",
    "    deep_branch = Dense(units=10, activation='relu')(deep_branch)  # Single dense layer\n",
    "\n",
    "    # Output Layer\n",
    "    final_branch = Dense(units=1, activation='sigmoid', name='deep_network')(deep_branch)\n",
    "\n",
    "    model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "deep_only_model = create_deep_only_model(feature_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 5ms/step\n",
      "26/26 [==============================] - 1s 4ms/step\n",
      "26/26 [==============================] - 1s 4ms/step\n",
      "26/26 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Embedding, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cross-Validation (k = 5)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results_model3 = []\n",
    "cv_results_deep_only = []\n",
    "\n",
    "for train_index, val_index in cv.split(data.drop('edible', axis=1), data['edible']):\n",
    "    X_train, X_val = data.drop('edible', axis=1).iloc[train_index], data.drop('edible', axis=1).iloc[val_index]\n",
    "    y_train, y_val = data['edible'].iloc[train_index], data['edible'].iloc[val_index]\n",
    "\n",
    "    ds_train = create_dataset_from_dataframe(pd.concat([X_train, y_train], axis=1))\n",
    "    ds_val = create_dataset_from_dataframe(pd.concat([X_val, y_val], axis=1))\n",
    "\n",
    "    # Create new instances of the models for each fold\n",
    "    model3 = create_wide_and_deep_model_with_depth(feature_space, deep_units=[10])  # Shallower model\n",
    "    deep_only_model = create_deep_only_model(feature_space)\n",
    "\n",
    "    # Train and evaluate Model 3\n",
    "    model3.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
    "    y_pred_proba_model3 = model3.predict(ds_val).flatten()\n",
    "    auc_model3 = roc_auc_score(y_val, y_pred_proba_model3)\n",
    "    cv_results_model3.append(auc_model3)\n",
    "\n",
    "    # Train and evaluate Deep-only model\n",
    "    deep_only_model.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
    "    y_pred_proba_deep_only = deep_only_model.predict(ds_val).flatten()\n",
    "    auc_deep_only = roc_auc_score(y_val, y_pred_proba_deep_only)\n",
    "    cv_results_deep_only.append(auc_deep_only)\n",
    "\n",
    "# Statistical Comparison (Paired T-test)\n",
    "_, p_value = stats.ttest_rel(cv_results_model3, cv_results_deep_only)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Credit to Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC - Model 3 (Wide & Deep): 0.9998986709838487\n",
      "Average AUC - Deep-Only Model: 0.9995527944651499\n"
     ]
    }
   ],
   "source": [
    "# Some averages for the AUC for each model as well for further evidence\n",
    "average_auc_model3 = np.mean(cv_results_model3)\n",
    "average_auc_deep_only = np.mean(cv_results_deep_only)\n",
    "\n",
    "print(\"Average AUC - Model 3 (Wide & Deep):\", average_auc_model3)\n",
    "print(\"Average AUC - Deep-Only Model:\", average_auc_deep_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the performance of our best wide and deep network (model 3 with only one deep layer) to a deep-only network with a similar structure but without the wide branch. We used k-fold cross-validation and the area under the ROC curve (AUC) as the evaluation metric, did ran a paired t-test on the cross-validation results, which revealed a statistically significant difference in performance (p-value = 0.02040387), with the wide and deep model achieving a higher average AUC score (0.999911) compared to the deep-only model (0.999726).\n",
    "\n",
    "While the wide and deep model showed statistically significant improvement, it is also important to note that it is a more complex model than our deep-only network. This model could be used for more lightweight applications, such as for mobile devices running our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be speeding up processing using a Custom Pre-Processor by employing the use of pre-processing in tf.data records like with did towards the end of the machine learning notebook provided 10a. Keras Wide and Deep as TFData on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used from MachineLearningNotebooks \"10a. Keras Wide and Deep as TFData.ipynp\"\n",
    "# Adapted to accomodate our specific Mushroom dataset.\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.utils import FeatureSpace\n",
    "\n",
    "# Crossing columns together \n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        # Categorical feature encoded as string\n",
    "        \"cap-shape\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"cap-surface\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"cap-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"bruises\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"odor\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-attachment\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-spacing\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-size\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"gill-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-shape\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-root\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-surface-above-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-surface-below-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-color-above-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"stalk-color-below-ring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"veil-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"ring-number\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"ring-type\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"spore-print-color\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"population\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"habitat\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "    },\n",
    "    # Specify feature cross with a custom crossing dim\n",
    "    crosses=[\n",
    "        # Cap-Color and Cap Shape\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('cap-color', 'cap-shape'),\n",
    "            crossing_dim= 10*6),\n",
    "        # Odor and Gill-Color\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('gill-color', 'odor'),\n",
    "            crossing_dim= 12*9),\n",
    "        # Bruises and Stalk-Surface-above-ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('bruises', 'stalk-surface-above-ring'),\n",
    "            crossing_dim= 2*4),\n",
    "        # Bruises and Stalk-Surface-below-ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('bruises', 'stalk-surface-below-ring'),\n",
    "            crossing_dim= 2*4),\n",
    "        # Ring-Type and Stalk-Color-below ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('ring-type', 'stalk-color-below-ring'),\n",
    "            crossing_dim= 8*8),\n",
    "        # Ring-Type and Stalk-Color-above ring\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('ring-type', 'stalk-color-above-ring'),\n",
    "            crossing_dim= 8*8),\n",
    "        # Spore-Print Color and Habitat\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('spore-print-color', 'habitat'),\n",
    "            crossing_dim= 9*7)\n",
    "    ],\n",
    "    output_mode=\"dict\",\n",
    ")\n",
    "\n",
    "# add explanation of this pre-processing here\n",
    "train_ds_with_no_labels = ds_train.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_with_no_labels)\n",
    "\n",
    "preprocessed_ds_train = ds_train.map(lambda x, y: (feature_space(x), y), \n",
    "                                     num_parallel_calls=tf.data.AUTOTUNE)\n",
    "preprocessed_ds_train = preprocessed_ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "preprocessed_ds_test = ds_test.map(lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "preprocessed_ds_test = preprocessed_ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)       [(None, 9)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)       [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)       [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)        [(None, 12)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)       [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)       [(None, 5)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)       [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)       [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)       [(None, 9)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)       [(None, 9)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)       [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)       [(None, 3)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)       [(None, 5)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)       [(None, 9)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)       [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)       [(None, 7)]                  0         []                            \n",
      "                                                                                                  \n",
      " cap-shape_embed (Embedding  (None, 6, 2)                 12        ['input_6[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " cap-surface_embed (Embeddi  (None, 4, 2)                 8         ['input_7[0][0]']             \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " cap-color_embed (Embedding  (None, 10, 3)                30        ['input_4[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bruises_embed (Embedding)   (None, 2, 1)                 2         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " odor_embed (Embedding)      (None, 9, 3)                 27        ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " gill-attachment_embed (Emb  (None, 2, 1)                 2         ['input_8[0][0]']             \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " gill-spacing_embed (Embedd  (None, 2, 1)                 2         ['input_12[0][0]']            \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " gill-size_embed (Embedding  (None, 2, 1)                 2         ['input_11[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " gill-color_embed (Embeddin  (None, 12, 3)                36        ['input_9[0][0]']             \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " stalk-shape_embed (Embeddi  (None, 2, 1)                 2         ['input_25[0][0]']            \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " stalk-root_embed (Embeddin  (None, 5, 2)                 10        ['input_24[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " stalk-surface-above-ring_e  (None, 4, 2)                 8         ['input_26[0][0]']            \n",
      " mbed (Embedding)                                                                                 \n",
      "                                                                                                  \n",
      " stalk-surface-below-ring_e  (None, 4, 2)                 8         ['input_27[0][0]']            \n",
      " mbed (Embedding)                                                                                 \n",
      "                                                                                                  \n",
      " stalk-color-above-ring_emb  (None, 9, 3)                 27        ['input_22[0][0]']            \n",
      " ed (Embedding)                                                                                   \n",
      "                                                                                                  \n",
      " stalk-color-below-ring_emb  (None, 9, 3)                 27        ['input_23[0][0]']            \n",
      " ed (Embedding)                                                                                   \n",
      "                                                                                                  \n",
      " veil-color_embed (Embeddin  (None, 4, 2)                 8         ['input_28[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " ring-number_embed (Embeddi  (None, 3, 1)                 3         ['input_16[0][0]']            \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " ring-type_embed (Embedding  (None, 5, 2)                 10        ['input_17[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " spore-print-color_embed (E  (None, 9, 3)                 27        ['input_20[0][0]']            \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " population_embed (Embeddin  (None, 6, 2)                 12        ['input_15[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " habitat_embed (Embedding)   (None, 7, 2)                 14        ['input_13[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_385 (Flatten)       (None, 12)                   0         ['cap-shape_embed[1][0]']     \n",
      "                                                                                                  \n",
      " flatten_386 (Flatten)       (None, 8)                    0         ['cap-surface_embed[1][0]']   \n",
      "                                                                                                  \n",
      " flatten_387 (Flatten)       (None, 30)                   0         ['cap-color_embed[1][0]']     \n",
      "                                                                                                  \n",
      " flatten_388 (Flatten)       (None, 2)                    0         ['bruises_embed[1][0]']       \n",
      "                                                                                                  \n",
      " flatten_389 (Flatten)       (None, 27)                   0         ['odor_embed[1][0]']          \n",
      "                                                                                                  \n",
      " flatten_390 (Flatten)       (None, 2)                    0         ['gill-attachment_embed[1][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " flatten_391 (Flatten)       (None, 2)                    0         ['gill-spacing_embed[1][0]']  \n",
      "                                                                                                  \n",
      " flatten_392 (Flatten)       (None, 2)                    0         ['gill-size_embed[1][0]']     \n",
      "                                                                                                  \n",
      " flatten_393 (Flatten)       (None, 36)                   0         ['gill-color_embed[1][0]']    \n",
      "                                                                                                  \n",
      " flatten_394 (Flatten)       (None, 2)                    0         ['stalk-shape_embed[1][0]']   \n",
      "                                                                                                  \n",
      " flatten_395 (Flatten)       (None, 10)                   0         ['stalk-root_embed[1][0]']    \n",
      "                                                                                                  \n",
      " flatten_396 (Flatten)       (None, 8)                    0         ['stalk-surface-above-ring_emb\n",
      "                                                                    ed[1][0]']                    \n",
      "                                                                                                  \n",
      " flatten_397 (Flatten)       (None, 8)                    0         ['stalk-surface-below-ring_emb\n",
      "                                                                    ed[1][0]']                    \n",
      "                                                                                                  \n",
      " flatten_398 (Flatten)       (None, 27)                   0         ['stalk-color-above-ring_embed\n",
      "                                                                    [1][0]']                      \n",
      "                                                                                                  \n",
      " flatten_399 (Flatten)       (None, 27)                   0         ['stalk-color-below-ring_embed\n",
      "                                                                    [1][0]']                      \n",
      "                                                                                                  \n",
      " flatten_400 (Flatten)       (None, 8)                    0         ['veil-color_embed[1][0]']    \n",
      "                                                                                                  \n",
      " flatten_401 (Flatten)       (None, 3)                    0         ['ring-number_embed[1][0]']   \n",
      "                                                                                                  \n",
      " flatten_402 (Flatten)       (None, 10)                   0         ['ring-type_embed[1][0]']     \n",
      "                                                                                                  \n",
      " flatten_403 (Flatten)       (None, 27)                   0         ['spore-print-color_embed[1][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " flatten_404 (Flatten)       (None, 12)                   0         ['population_embed[1][0]']    \n",
      "                                                                                                  \n",
      " flatten_405 (Flatten)       (None, 14)                   0         ['habitat_embed[1][0]']       \n",
      "                                                                                                  \n",
      " embed_concat (Concatenate)  (None, 277)                  0         ['flatten_385[1][0]',         \n",
      "                                                                     'flatten_386[1][0]',         \n",
      "                                                                     'flatten_387[1][0]',         \n",
      "                                                                     'flatten_388[1][0]',         \n",
      "                                                                     'flatten_389[1][0]',         \n",
      "                                                                     'flatten_390[1][0]',         \n",
      "                                                                     'flatten_391[1][0]',         \n",
      "                                                                     'flatten_392[1][0]',         \n",
      "                                                                     'flatten_393[1][0]',         \n",
      "                                                                     'flatten_394[1][0]',         \n",
      "                                                                     'flatten_395[1][0]',         \n",
      "                                                                     'flatten_396[1][0]',         \n",
      "                                                                     'flatten_397[1][0]',         \n",
      "                                                                     'flatten_398[1][0]',         \n",
      "                                                                     'flatten_399[1][0]',         \n",
      "                                                                     'flatten_400[1][0]',         \n",
      "                                                                     'flatten_401[1][0]',         \n",
      "                                                                     'flatten_402[1][0]',         \n",
      "                                                                     'flatten_403[1][0]',         \n",
      "                                                                     'flatten_404[1][0]',         \n",
      "                                                                     'flatten_405[1][0]']         \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 108)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)       [(None, 64)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)       [(None, 64)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)       [(None, 63)]                 0         []                            \n",
      "                                                                                                  \n",
      " deep1 (Dense)               (None, 50)                   13900     ['embed_concat[1][0]']        \n",
      "                                                                                                  \n",
      " cap-color_X_cap-shape_embe  (None, 60, 7)                420       ['input_5[0][0]']             \n",
      " d (Embedding)                                                                                    \n",
      "                                                                                                  \n",
      " gill-color_X_odor_embed (E  (None, 108, 10)              1080      ['input_10[0][0]']            \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " bruises_X_stalk-surface-ab  (None, 8, 2)                 16        ['input_2[0][0]']             \n",
      " ove-ring_embed (Embedding)                                                                       \n",
      "                                                                                                  \n",
      " bruises_X_stalk-surface-be  (None, 8, 2)                 16        ['input_3[0][0]']             \n",
      " low-ring_embed (Embedding)                                                                       \n",
      "                                                                                                  \n",
      " ring-type_X_stalk-color-be  (None, 64, 8)                512       ['input_19[0][0]']            \n",
      " low-ring_embed (Embedding)                                                                       \n",
      "                                                                                                  \n",
      " ring-type_X_stalk-color-ab  (None, 64, 8)                512       ['input_18[0][0]']            \n",
      " ove-ring_embed (Embedding)                                                                       \n",
      "                                                                                                  \n",
      " spore-print-color_X_habita  (None, 63, 7)                441       ['input_21[0][0]']            \n",
      " t_embed (Embedding)                                                                              \n",
      "                                                                                                  \n",
      " deep2 (Dense)               (None, 25)                   1275      ['deep1[1][0]']               \n",
      "                                                                                                  \n",
      " flatten_378 (Flatten)       (None, 420)                  0         ['cap-color_X_cap-shape_embed[\n",
      "                                                                    1][0]']                       \n",
      "                                                                                                  \n",
      " flatten_379 (Flatten)       (None, 1080)                 0         ['gill-color_X_odor_embed[1][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " flatten_380 (Flatten)       (None, 16)                   0         ['bruises_X_stalk-surface-abov\n",
      "                                                                    e-ring_embed[1][0]']          \n",
      "                                                                                                  \n",
      " flatten_381 (Flatten)       (None, 16)                   0         ['bruises_X_stalk-surface-belo\n",
      "                                                                    w-ring_embed[1][0]']          \n",
      "                                                                                                  \n",
      " flatten_382 (Flatten)       (None, 512)                  0         ['ring-type_X_stalk-color-belo\n",
      "                                                                    w-ring_embed[1][0]']          \n",
      "                                                                                                  \n",
      " flatten_383 (Flatten)       (None, 512)                  0         ['ring-type_X_stalk-color-abov\n",
      "                                                                    e-ring_embed[1][0]']          \n",
      "                                                                                                  \n",
      " flatten_384 (Flatten)       (None, 441)                  0         ['spore-print-color_X_habitat_\n",
      "                                                                    embed[1][0]']                 \n",
      "                                                                                                  \n",
      " deep3 (Dense)               (None, 10)                   260       ['deep2[1][0]']               \n",
      "                                                                                                  \n",
      " wide_concat (Concatenate)   (None, 2997)                 0         ['flatten_378[1][0]',         \n",
      "                                                                     'flatten_379[1][0]',         \n",
      "                                                                     'flatten_380[1][0]',         \n",
      "                                                                     'flatten_381[1][0]',         \n",
      "                                                                     'flatten_382[1][0]',         \n",
      "                                                                     'flatten_383[1][0]',         \n",
      "                                                                     'flatten_384[1][0]']         \n",
      "                                                                                                  \n",
      " concat_deep_wide (Concaten  (None, 3007)                 0         ['deep3[1][0]',               \n",
      " ate)                                                                'wide_concat[1][0]']         \n",
      "                                                                                                  \n",
      " combined (Dense)            (None, 1)                    3008      ['concat_deep_wide[1][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21717 (84.83 KB)\n",
      "Trainable params: 21717 (84.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def setup_embedding_from_encoding(encoded_features, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    \n",
    "    # get the size of the feature\n",
    "    x = encoded_features[col_name]\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "    \n",
    "    \n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "    \n",
    "    return x\n",
    "\n",
    "dict_inputs = feature_space.get_inputs() # need to use unprocessed features here, to gain access to each output\n",
    "encoded_features = feature_space.get_encoded_features() # these features have been encoded\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# for each crossed variable, make an embedding\n",
    "for col in feature_space.crossers.keys():\n",
    "    \n",
    "    x = setup_embedding_from_encoding(encoded_features, col)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# for each categorical variable\n",
    "for col in categorical_headers:\n",
    "    \n",
    "    # get the output tensor from ebedding layer\n",
    "    x = setup_embedding_from_encoding(encoded_features, col)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# merge the deep branches together\n",
    "deep_branch = Concatenate(name='embed_concat')(all_deep_branch_outputs)\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "# encoded features input, fast\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=final_branch)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "training_model.summary()\n",
    "\n",
    "# non-encoded, perform redundant operations\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "inference_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "102/102 - 5s - loss: 0.6063 - accuracy: 0.7268 - val_loss: 0.8443 - val_accuracy: 0.4812 - 5s/epoch - 52ms/step\n",
      "Epoch 2/10\n",
      "102/102 - 1s - loss: 0.6531 - accuracy: 0.6332 - val_loss: 0.4808 - val_accuracy: 0.8769 - 1s/epoch - 13ms/step\n",
      "Epoch 3/10\n",
      "102/102 - 1s - loss: 0.2132 - accuracy: 0.9266 - val_loss: 0.1084 - val_accuracy: 0.9680 - 1s/epoch - 11ms/step\n",
      "Epoch 4/10\n",
      "102/102 - 1s - loss: 0.0612 - accuracy: 0.9812 - val_loss: 0.0969 - val_accuracy: 0.9680 - 1s/epoch - 11ms/step\n",
      "Epoch 5/10\n",
      "102/102 - 1s - loss: 0.0930 - accuracy: 0.9760 - val_loss: 0.0660 - val_accuracy: 0.9846 - 1s/epoch - 12ms/step\n",
      "Epoch 6/10\n",
      "102/102 - 1s - loss: 0.0598 - accuracy: 0.9862 - val_loss: 0.0387 - val_accuracy: 0.9938 - 1s/epoch - 12ms/step\n",
      "Epoch 7/10\n",
      "102/102 - 1s - loss: 0.0264 - accuracy: 0.9931 - val_loss: 0.0369 - val_accuracy: 0.9920 - 1s/epoch - 12ms/step\n",
      "Epoch 8/10\n",
      "102/102 - 1s - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.0077 - val_accuracy: 0.9994 - 1s/epoch - 12ms/step\n",
      "Epoch 9/10\n",
      "102/102 - 1s - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9994 - 1s/epoch - 11ms/step\n",
      "Epoch 10/10\n",
      "102/102 - 1s - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0019 - val_accuracy: 0.9994 - 1s/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "history = training_model.fit(\n",
    "    preprocessed_ds_train, epochs=10, validation_data=preprocessed_ds_test, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging tf.data for dataset handling and manipulation, the code efficiently processes large datasets asynchronously and in parallel, which is crucial for high-throughput machine learning tasks. The use of FeatureSpace for defining categorical transformations and feature crosses directly in the TensorFlow graph optimizes the preprocessing steps, reducing overhead and improving runtime performance during both training and inference. As we seen when running this code the the preprocessing time was way less and produced similar results to our models before.\n",
    "\n",
    "The code also makes extensive use of batch processing and prefetching (tf.data.AUTOTUNE), which ensures that data loading does not become a bottleneck. Data is prepared and available for the model as soon as it needs it, which is critical for maintaining high training speeds, especially on large datasets like the one we are working with.\n",
    "\n",
    "Overall, this approach of employing TensorFlow’s advanced features for efficient data handling, processing, and model training is well-suited for our task of classifying mushrooms as edible or poisonous. This method not only maximizes performance but also ensures scalability and adaptability of the machine learning pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
