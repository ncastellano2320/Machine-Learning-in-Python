{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 6: Convolutional Neural Networks**\n",
    "### Authors: Will Lahners, Edward Powers, and Nino Castellano\n",
    "\n",
    "## **Describing the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine learning project presents a compelling business case, leveraging a comprehensive dataset comprising cars and motorcycles. The rationale behind this initiative is to cater to the pressing needs of law enforcement agencies, state departments of transportation, and similar entities, all of which rely heavily on efficient vehicle identification systems. By leveraging advanced algorithms, our software will empower police stations to expedite investigations, identify stolen vehicles, and enforce traffic regulations with greater precision. Similarly, state departments of transportation can streamline processes related to vehicle registration, licensing, and compliance, thereby reducing administrative burdens and enhancing overall operational efficiency. By offering a sophisticated solution tailored to the specific needs of these agencies, we not only create value for them but also contribute to the broader goal of promoting road safety and effective law enforcement practices. Hence, our machine learning project represents a strategic investment with far-reaching benefits for both public safety and administrative effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (3 points total)\n",
    "\n",
    "> [1.5 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Recommended Metric: F1-Score\n",
    "In this scenario, both precision and recall are important:\n",
    "\n",
    "Precision: We want to ensure that when the model identifies a vehicle as a car, it is indeed a car. Misclassifying a bike as a car could lead to incorrect traffic light timing or inaccurate traffic flow analysis. \n",
    "This could result in inefficient traffic management strategies and potentially hazardous road conditions. Additionally, precise identification is crucial for law enforcement purposes, \n",
    "where misclassification could lead to erroneous criminal investigations or traffic violation penalties.\n",
    "\n",
    "Recall: We also want to ensure that the model correctly identifies most cars. Missing cars could lead to underestimation of traffic volume and congestion, impacting traffic management decisions. \n",
    "A high recall rate is essential for accurately assessing the flow of vehicles on roads, ensuring that transportation authorities can effectively plan infrastructure improvements and allocate resources accordingly. \n",
    "Moreover, in law enforcement scenarios, comprehensive vehicle identification is necessary for detecting stolen vehicles or identifying suspects involved in criminal activities.\n",
    "\n",
    "The F1-score balances precision and recall, making it a suitable choice when both aspects are crucial. It provides a single score that reflects the overall \n",
    "accuracy and robustness of the model, enabling stakeholders in law enforcement agencies, state departments of transportation, and similar entities to assess the performance of the vehicle identification software comprehensively.\n",
    "By prioritizing both precision and recall through the F1-score, we ensure that our software solution meets the stringent standards of reliability, accuracy, and effectiveness demanded by our business case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "We've chosen stratified K-fold cross-validation with K=10 to assess our car vs. bike classification model. This method ensures diverse testing data, mimicking real-world scenarios of continuous data streams.\n",
    "It also addresses class imbalances, crucial for fair evaluation. By repeatedly training and testing the model across multiple folds, we simulate real-world deployment, boosting confidence in its generalization ability.\n",
    "This approach provides a robust and realistic evaluation mirroring practical usage, vital for ensuring the model's reliability and effectiveness in deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (6 points total)\n",
    "\n",
    "> [1.5 points]  Setup the training to use data expansion in Keras (also called data augmentation). Explain why the chosen data expansion techniques are appropriate for your dataset. You should make use of Keras augmentation layers, like in the class examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Model 1A \n",
    "In implementing data augmentation techniques using Keras for our car vs. bike classification model, we've chosen methods like horizontal flipping, random rotation, zoom, and contrast adjustment that align with our model \n",
    "architecture and business case requirements. These techniques play a pivotal role in enhancing the model's generalization ability by increasing the variability and diversity of the training data. By introducing random \n",
    "transformations, we effectively mimic real-world scenarios, accounting for variations in lighting, angles, and perspectives encountered in image data. This is particularly crucial for our task, where accurately distinguishing \n",
    "between cars and bikes requires robustness to such environmental factors. Moreover, these augmentation strategies are well-suited for our convolutional neural network (CNN) architecture. By applying transformations before \n",
    "the data passes through the convolutional layers, we equip the model with the capability to learn from a more diverse set of training examples. This improves its ability to extract meaningful features regardless of variations \n",
    "in the orientation or scale of vehicles in the images. The reported training results showcase the effectiveness of these techniques in progressively enhancing the model's performance metrics, including accuracy, precision, \n",
    "recall, and area under the ROC curve (AUC), across multiple folds of the data. Additionally, analysis of the training graphs reveals a consistent trend of increasing average training accuracy with additional epochs, indicating \n",
    "the continuous improvement of the model's learning process over time. Overall, the integration of data augmentation techniques contributes significantly to the model's robustness and its capacity to generalize to unseen data, \n",
    "aligning with the requirements of our business case.\n",
    "\n",
    "Model 2A Breakdown: For Model 1B, we continue to employ data augmentation techniques using Keras, comprising horizontal flipping, random rotation, zoom, and contrast adjustment. These techniques are selected to align \n",
    "with our model architecture and business case needs, aiming to enhance the model's capacity to generalize across diverse datasets. By introducing random transformations, we effectively simulate real-world conditions, \n",
    "accounting for variations in lighting, angles, and perspectives present in image data. This is vital for accurately distinguishing between cars and bikes, where robustness to such environmental factors is paramount for \n",
    "reliable classification. Furthermore, these augmentation strategies complement the CNN architecture of Model 1B, enhancing its ability to extract meaningful features from images. By applying transformations before data \n",
    "passes through convolutional layers, we ensure that the model learns from a broader spectrum of training examples, thereby improving its robustness. The reported training results indicate a progressive improvement in \n",
    "performance metrics such as accuracy, precision, recall, and area under the ROC curve (AUC) across multiple folds of the data. Additionally, analysis of training graphs reveals a consistent trend of increasing average \n",
    "training accuracy with additional epochs, affirming the continuous refinement of the model's learning process over time. Overall, the integration of data augmentation techniques in Model 1B contributes significantly to \n",
    "its robustness and generalization capacity, aligning effectively with the requirements of our business case.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [2 points] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures and investigate changing one or more parameters of each architecture such as the number of filters. This means, at a  minimum, you will train a total of four models (2 different architectures, with 2 parameters changed in each architecture). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras). Be sure that models converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1.5 points] Visualize the final results of all the CNNs and interpret/compare the performances. Use proper statistics as appropriate, especially for comparing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
