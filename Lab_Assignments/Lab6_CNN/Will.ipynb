{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 6: Convolutional Neural Networks**\n",
    "### Authors: Will Lahners, Edward Powers, and Nino Castellano\n",
    "\n",
    "## **Describing the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (3 points total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (images): (3255, 224, 224, 3)\n",
      "Shape of y (labels): (3255,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(folder_path, target_size=(224, 224)):\n",
    "    X = []  # List to store image data\n",
    "    y = []  # List to store labels\n",
    "    \n",
    "    for character_folder in os.listdir(folder_path):\n",
    "        character_path = os.path.join(folder_path, character_folder)\n",
    "        \n",
    "        for image_name in os.listdir(character_path):\n",
    "            image_path = os.path.join(character_path, image_name)\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, target_size)\n",
    "            \n",
    "            # Convert the image to grayscale if needed (assuming you're using grayscale images)\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Preprocess the image if needed (normalize, etc.)\n",
    "            # image = preprocess_image(image)\n",
    "            \n",
    "            # Extract the label from the folder name\n",
    "            label = character_folder\n",
    "            \n",
    "            # Append the image and label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Define the folder path where your dataset is located\n",
    "folder_path = 'one-piece/Data/'\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_dataset(folder_path)\n",
    "\n",
    "# Print the shape of the loaded data\n",
    "print(\"Shape of X (images):\", X.shape)\n",
    "print(\"Shape of y (labels):\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1.5 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train set size: 2929, Test set size: 326\n",
      "Fold 2: Train set size: 2929, Test set size: 326\n",
      "Fold 3: Train set size: 2929, Test set size: 326\n",
      "Fold 4: Train set size: 2929, Test set size: 326\n",
      "Fold 5: Train set size: 2929, Test set size: 326\n",
      "Fold 6: Train set size: 2930, Test set size: 325\n",
      "Fold 7: Train set size: 2930, Test set size: 325\n",
      "Fold 8: Train set size: 2930, Test set size: 325\n",
      "Fold 9: Train set size: 2930, Test set size: 325\n",
      "Fold 10: Train set size: 2930, Test set size: 325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "\n",
    "# Initialize StratifiedKFold object\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store train and test indices for each fold\n",
    "train_indices_list = []\n",
    "test_indices_list = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "for train_indices, test_indices in stratified_kfold.split(X, y):\n",
    "    train_indices_list.append(train_indices)\n",
    "    test_indices_list.append(test_indices)\n",
    "\n",
    "# Now train_indices_list and test_indices_list contain the indices for each fold\n",
    "\n",
    "# Example usage:\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(zip(train_indices_list, test_indices_list)):\n",
    "    print(f\"Fold {fold_idx + 1}: Train set size: {len(train_idx)}, Test set size: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the stratified 10-fold cross validation because we have multiple classes. By using this strafified 10-fold validation, we will be able to ensure that each class if represented equally in both the training and testing sets. It will maintain the same class distribution in each fold as the original dataset, which is great for us our model as it will generalize well to unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (6 points total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1.5 points]  Setup the training to use data expansion in Keras (also called data augmentation). Explain why the chosen data expansion techniques are appropriate for your dataset. You should make use of Keras augmentation layers, like in the class examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [2 points] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures and investigate changing one or more parameters of each architecture such as the number of filters. This means, at a  minimum, you will train a total of four models (2 different architectures, with 2 parameters changed in each architecture). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras). Be sure that models converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1.5 points] Visualize the final results of all the CNNs and interpret/compare the performances. Use proper statistics as appropriate, especially for comparing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
